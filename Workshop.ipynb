{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eae8d67-75ae-4a51-bf92-ca493418452e",
   "metadata": {},
   "source": [
    "# HPA Workshop: Tasmanian Devil Cancer Analysis\n",
    "\n",
    "Welcome to this workshop!\n",
    "\n",
    "In the process of this session, you will:\n",
    "* Get hands-on experience working processing some real cancer data\n",
    "* Have a basic introduction to supervised neural networks\n",
    "* * Play around with different ways to get information into the network\n",
    "* Be introduced to some un-supervised techniques for anomaly detection\n",
    "\n",
    "\n",
    "There is a lot of text and explanations on this notebook - you can process them at your own speeds (or ignore them entirely). The following key is used:\n",
    "1. Text in black is background information\n",
    "2. <font color='red'>**Text in red is a task you should complete**</font>\n",
    "3. <font color='blue'>**Text in blue is a question to think about as a group**</font>\n",
    "\n",
    "## Quick Recap: Tasmanian Devils\n",
    "\n",
    "To briefly recap Zemin's presentation:\n",
    "1. Tasmanian Devils suffer from a form of transmissible cancer, which gives them 'Devil Facial Disease' (DFTD)\n",
    "2. This cancer shows signs of `chromothripsis', a catastrophic genomic rearrangement\n",
    "3. In addition to rearrangement, there are signs of large scale deletions and duplications of sections of the DNA\n",
    "4. Chromothripsis shows similar patterns in other forms of cancer; human oesophogeal and prostate cancer, for example\n",
    "## Our Goals\n",
    "\n",
    "Our research (and your task today) is to find a way to classify and study this cancer -- in the hope that we may then extend it from this easier case<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) to studying the more complex forms.\n",
    "\n",
    "To do this, we must first:\n",
    "1. Label the data (this has been done for us!)\n",
    "2. Process the data into useable form\n",
    "3. Set-up and then train a Neural Network Classifier\n",
    "4. Utilise some form of un-supervised methods to detect variations that are not contained within our labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) Tasmanian Devils went through a genetic bottleneck (hence the transmissible cancer), and so they are all genetically very similar. This makes their genomes relatively homogeneous with respect to one another"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6568332-ef6c-4862-ac0b-6d45f289e033",
   "metadata": {},
   "source": [
    "## Coverage Data\n",
    "Let's first look at some coverage data. \n",
    "\n",
    "It helps to first understand why duplications/deletions result in coverage variations:\n",
    "\n",
    "<img src=\"Images/coverageExplain.png\" />\n",
    "\n",
    "### Our Data\n",
    "\n",
    "The raw datafiles for several Tasmanian Devils is found in the Data directory. We have given the Devils human names to aid in communication.\n",
    "\n",
    "((List some known Devils so they can browse))\n",
    "\n",
    "When we come to do this for real, we would work on the entire genome at once. However, for ease of use we have pre-selected the portions of interest to us. The datafiles represent a `super chromosome' formed by joining Chromosomes 4 and 5 into one long chain. This is an artifice to make this workshop run easier!\n",
    "\n",
    "\n",
    "<font color='red'>**Run the code below, varying the dataThinning values and the data until you feel that you have a handle on the data**</font>\n",
    "\n",
    "<font color='red'>We have provided a small subset of the data for you to choose from:\n",
    "1. `Abigail.dat`\n",
    "2. `Diane.dat`\n",
    "3. `Travis.dat`\n",
    "4. `Elmer.dat`\n",
    "5. `Vern.dat`\n",
    "6. `Arthur.dat`\n",
    "\n",
    "These are all Tasmanian Devils - some with cancer, some without.\n",
    "\n",
    "There is also another dataset in there -- `Human.dat` -- this is a human oesophageal cancer sample.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dae59b-02a4-4a9a-ba50-16a915cd9d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import deforest\n",
    "file = \"Data/Human.dat\"\n",
    "\n",
    "dataThinning = 0.999 #This value controls the level of smoothing applied to the data. 0 is the raw data, 0.99(etc) is very smooth.\n",
    "baseSkipping = 1000 #This value controls the base-sampling rate. Higher values makes the later code run quicker, but uses less information\n",
    "s = deforest.DataStruct(file,dataThinning,baseSkipping) #We have provided an inbuilt data-parser for you, because we're nice.\n",
    "\n",
    "plt.figure(1)\n",
    "plt.clf() #comment this out if you do want to layer multiple plots\n",
    "plt.plot(s.Index,s.Coverage)\n",
    "plt.ylabel(\"Coverage\")\n",
    "plt.xlabel(\"Chromosome Index\")\n",
    "plt.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36551db6-cfe5-4b0a-8c39-34cdd5a69d50",
   "metadata": {},
   "source": [
    "<font color=blue>**Can you identify the differences between these samples?**</font>\n",
    "\n",
    "<font color=blue>**What problems can you think we might have to overcome, in order to get this data into a form useable for machine learning?**</font>\n",
    "\n",
    "\n",
    "\n",
    "## The Deforester\n",
    "\n",
    "To overcome some of these problems, we have developed the *deforester*, a Bayesian inference algorithm which:\n",
    "1. Determines the overall per-homolog coverage rate\n",
    "2. Finds the most likely positions for 'transitions'\n",
    "3. Encodes those transitions into a feature vector of variable length\n",
    "\n",
    "The key thing which makes *deforester* work is the knowledge that 'a transition' (i.e. a duplication or deletion event) should (assuming no subclonality) result in a coverage variation which is an **integer multiple of the per-homolog coverage rate**; we then draw analogies between the resonant frequencies ('harmonics') observed in musical instruments to create a powerful statistical machinery.\n",
    "\n",
    "The code takes three main parameters:\n",
    "1. The assumed statistical distribution of the model\n",
    "2. The maximum harmonic considered (this is purely a computational limit; the code runs quicker with this value smaller, but is more accurate with it larger!)\n",
    "3. The *smallest permitted jump*. This prevents the code from spuriously identifying small variations: we know (for example) that the affected regions must be larger than the read length to be detected via this method (else the alignment tool would catch it)\n",
    "\n",
    "<font color=red>**Explore the features of the provided probability distribution, and try coming up with your own**</font>\n",
    "\n",
    "<font color=red>*Your probability distribution function should:*\n",
    "1. *Be a function of the average coverage value (nu) and the measured coverage value of a given base (k)*\n",
    "2. *Be negative everywhere (probabilities are less than 1, and this is a log-probability)*\n",
    "3. *Not necessarily sum to 1 (the code self-normalises)*\n",
    "4. *Not do anything weird like got to negative infinity*\n",
    "5. *Be as weird as you like, the purpose of this is to demonstrate how varying the distribution function affects the output!*\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ddb26-2f38-4102-ad78-76136ed67ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jackLogProbability(k,nu):\n",
    "\t#This is a (somewhat) biologically motivated distribution function, which assumes that the distribution is Poisson \n",
    "\t#distributed around the norm, but with some statistical noise distributed according to the Gamma distribution; which \n",
    "\t#in turn means the distribution is a Negative Binomial distribution. \n",
    "\t#Then on *top* of that, I add a secondary noise term which is just a big gaussian \n",
    "\t#background noise centred on nu - representing a general background uncertainty in everything\n",
    "\t\n",
    "\tfractionOfDataWhichIsReal = 0.99\n",
    "\tdataNoise = 5\n",
    "\tbackgroundNoise = 10\n",
    "\treturn deforest.dualBinomial(k,nu,fractionOfDataWhichIsReal,dataNoise,backgroundNoise)\n",
    "\n",
    "def myLogProbability(k,nu):\n",
    "\tsigma = 5 \n",
    "\treturn -0.5 * ((k-nu)/sigma)**2  #this is just a boring Gaussian that I put down. You make up your own function. Get weird.\n",
    "\n",
    "nu = 45\n",
    "k = np.arange(0,100)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.clf()\n",
    "plt.plot(k,jackLogProbability(k,nu),label=\"Biological Function\")\n",
    "plt.plot(k,myLogProbability(k,nu),label=\"My Function\")\n",
    "plt.xlabel(\"Observed Coverage\")\n",
    "plt.legend()\n",
    "plt.ylabel(f\"Log-Probability with Mean {nu}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec679ab",
   "metadata": {},
   "source": [
    "## Running Deforester\n",
    "\n",
    "The deforester algorithm uses a network-navigation based approach to ensure that the predicted harmonics are the globally most-likely set, subject to the following constraints:\n",
    "* Each base in the sequence is a 'layer' in the network, which the q^th node being being associated with the q^th harmonic\n",
    "* If the final minimum-distance path passes through the q^th node in the i^th layer, then the i^th base is said to have a harmonic of q \n",
    "* Assigning a base with observed coverage `k` to the `q`th harmonic incurs a penalty equal to `myProbabilityFunction(k,q*nu)`, where `nu` is an inferred fundamental frequency\n",
    "* `nu` is determined by doing a rapid parameter search with resolution `SearchResolution`\n",
    "* A harmonic transition cannot occur within a distance `minimumJump` from another harmonic transition\n",
    "* The harmonic cannot exceed `maxHarmonic`\n",
    "* A penalty of `LogJumpPrior` is applied every time a harmonic transition is applied\n",
    "* A penalty of `LogDiploidPrior` is applied to every base which does not have a harmonic of 2<a name=\"cite_ref-2\"></a>[<sup>[1]</sup>](#cite_note-2)\n",
    "\n",
    "\n",
    "\n",
    "<a name=\"cite_note-2\"></a>1. [^](#cite_ref-2) Because we are only looking at a subset of the Devil Genome which is badly affected by cancer, we have actually reduced this requirement for this workshop. We only apply the penalty to harmonics which are not 2, 3 or 4\n",
    "\n",
    "<font color=red>**Run the deforester algorithm on a dataset of your choice - vary the parameters of the model, and see how that affects the outcome**</font>\n",
    "\n",
    "<font color=blue>**How does this help us achieve our goals? What was the purpose of this exercise?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5997779a-d683-4a9a-97a3-49524e5849bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"Data/Diane.dat\"\n",
    "\n",
    "preSmoothing = 0\n",
    "dataSkip = 5000 #if you are running on the web hosted version (it's very slow!)\n",
    "# dataSkip = 500 #if you are running locally\n",
    "maxHarmonic = 16\n",
    "minimumJump = 1e6\n",
    "\n",
    "\n",
    "s = deforest.DataStruct(file,preSmoothing,dataSkip)\n",
    "sPlot = deforest.DataStruct(file,0.999,dataSkip) #load a smoothed version for plotting!\n",
    "\n",
    "N = deforest.Network(maxHarmonic,minimumJump)\n",
    "N.LogJumpPrior = -50 #Set this to zero to permit more jumps, set to a large negative value (< -1000) to remove jumps that the code is less certain about\n",
    "N.LogDiploidPrior = -0.1 #Set this to zero to disable the Diploid Prior (for maximum impact, also increase maxHarmonic to see what happens!)\n",
    "N.SearchResolution = 50 # Turn this value down to make it faster, but too low and your results become nonsense as nu cannot accurately be determined\n",
    "optimalPath = N.Navigate(s,jackLogProbability)\n",
    "\n",
    "\n",
    "\n",
    "## Now we plot the model\n",
    "fig,axs = plt.subplots(1,3)\n",
    "axs[0].plot(sPlot.Index,sPlot.Coverage,'k',label=\"Data (thin= \" + str(preSmoothing) + \")\")\n",
    "[xx,yy] = optimalPath.GetPlottingPath(s) \n",
    "axs[0].plot(xx,yy,label=\"Harmonic Fit (thin= \" + str(preSmoothing) + \")\")\n",
    "axs[0].legend()\n",
    "axs[0].set_xlabel(\"Chromosome Index\")\n",
    "axs[0].set_ylabel(\"Coverage\")\n",
    "axs[1].plot(xx,yy/optimalPath.Nu)\n",
    "axs[1].set_xlabel(\"Chromosome Index\")\n",
    "axs[1].set_ylabel(\"Harmonic\")\n",
    "axs[2].plot(N.nus,N.scores)\n",
    "axs[2].set_xlabel(\"Fundamental Frequency (nu)\")\n",
    "axs[2].set_ylabel(\"Inferred Score\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a0e19d",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "\n",
    "Having extracted the most important features, we now turn to encoding this information into a *feature vector*, of length `N`. \n",
    "\n",
    "<font color=blue>**Why is it important to be able to specify `N`?**</font>\n",
    "\n",
    "For the sake of simplicity, we have provided 3 different ways to encode the information. All three work by segmenting the chromosome into `N` equal parts (and expressing the chromosome index as a fraction along the chromosome - a value between 0 and 1).\n",
    "\n",
    "1. Method one (`diff`) computes the value of $\\sum_i (q_{i-1} - q_{i+1})$ of every transition in that segment. It therefore counts the 'net jumps' between the sample values.\n",
    "2. Method two (`sum`) computes the value of $\\sum_i |q_{i-1} - q_{i+1}|$ of every transition in that segment. The absolute value ensures that a jump from `q=1` to `q=2`, back to `q=1` scores a value of 2, rather than 0 (as `diff` would score it)\n",
    "3. Method three (`sqdiff`) computes the value of $\\sum_i (q_{i-1} - q_{i+1})^2$ across the segment. \n",
    "\n",
    "<font color=blue>**What benefits do these different encodings bring? Why might one be better than the other? Can you think of your own, better encoding?**</font>\n",
    "\n",
    "<font color=red>**Try generating the encoding for your chosen datafile -- vary the resolution and see if your earlier guesses hold up.**</font>\n",
    "\n",
    "*I will note that there does seem to be something odd about the behaviour of `sqdiff`, it doesn't seem to do what I thought it did (the values are smaller than `sum` which shouldn't be the case!) -- I spotted it too late to correct for this presentation, is it still encoding the relevant information, despite this error?* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8c967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(1,3)\n",
    "modes = ['diff','sum','sqdiff']\n",
    "for i in range(len(modes)):\n",
    "\tenc = optimalPath.Encode(s,N.JumpSize,modes[i],10)\n",
    "\taxs[i].plot(np.linspace(0,1,len(enc)),enc)\n",
    "\taxs[i].set_title(modes[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3a4399",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Onto the exciting bit!\n",
    "\n",
    "Now that we have a rigorous method for turning coverage datafiles into encoded feature vectors, we are going to attempt to build a neural network classifier which can tell us the probability that a provided encoding falls into one of three categories: non-cancerous (category 0), type-1 DFTD (category 1), or type-2 DFTD (category 2).  \n",
    "\n",
    "We will first do this using a **supervised** method. A supervised classifier 'trains' itself by using a pre-labelled dataset. \n",
    "\n",
    "We have provided a number of different such datasets, in the `ProcessedData` repository. These contain 172 other Tasmanian Devils (**the 6 Tasmanian Devils you have data for are not included in this sample**).\n",
    "\n",
    "1. `AllData` includes 172 Tasmanian Devils processed using `deforester` with 45 different parameter combinations\n",
    "2. `HighConfidence` is a subset of 20 parameter combinations of `AllData`, where the probability model is *not* dominated by the background noise\n",
    "3. `BigGap` is a subset of 8 parameter combinations of `HighConfidence` where the `minimumJump` value is larger than 1 million bases\n",
    "4. `SmallGap` is the corresponding subset of 12 parameter combinations where `minimumJump` is smaller than 1 million bases\n",
    "\n",
    "For each of these subsets there are then a number of different encodings, at a number of different resolutions - varying from 10-dimensional, to 1000-dimensional.\n",
    "\n",
    "<font color=blue>**Why do we have so many different options? What is the purpose of the multiple parameter models? Which do you expect to perform the best? What is the difference between validation data and training data?**</font>\n",
    "\n",
    "\n",
    "<font color=red>**Choose the datafile you are going to train, and examine the feature and label vectors**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c4220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTrainingData(file,trainingFraction):\n",
    "\n",
    "\tload_x = []\n",
    "\tload_y = []\n",
    "\tnames = []\n",
    "\twith open(file) as f:\n",
    "\t\tfor line in f:\t\n",
    "\t\t\tvals = line.rstrip().split(' ')\n",
    "\t\t\tx = []\n",
    "\n",
    "\t\t\t\n",
    "\t\t\tparams = vals[0].split('_')\n",
    "\t\t\tname = params[0]\n",
    "\t\t\t## if you want to get fancy, you can additionally provide the network with the deforester parameters which were used to generate it\n",
    "\t\t\t## uncomment these lines to add three dimensions to your feature vector -- but remember that you'll need to do that for your encoding later on!\n",
    "\t\t\t# gap = int(params[1])\n",
    "\t\t\t# noise = float(params[2])/10000\n",
    "\t\t\t# sigma = float(params[3])\n",
    "\t\t\t# x = [gap,noise,sigma]\n",
    "\t\t\tfor i in range(2,len(vals)):\n",
    "\t\t\t\tx.append(float(vals[i]))\n",
    "\t\t\ttag = int(vals[1])\t\t\t\n",
    "\t\t\tload_x.append(x)\n",
    "\t\t\tload_y.append(tag)\n",
    "\t\t\tnames.append(name)\n",
    "\tN = len(load_x)\n",
    "\tif trainingFraction < 1.0:\n",
    "\t\tp = np.random.permutation(N)\n",
    "\t\tload_x = np.array(load_x)[p]\n",
    "\t\tload_y = np.array(load_y)[p]\n",
    "\n",
    "\ttrainN  = int(trainingFraction*N)\n",
    "\n",
    "\treturn (load_x[:trainN],load_y[:trainN]), (load_x[trainN:],load_y[trainN:]), names\n",
    "\n",
    "trainingFile = \"ProcessedData/AllData/Res_100_sum.dat\"\n",
    "(x_train, y_train), (x_test, y_test),names = loadTrainingData(trainingFile,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc1bd6b",
   "metadata": {},
   "source": [
    "## Creating the network\n",
    "\n",
    "For the case of a simple classifier, we are going to construct a very basic Feedforward Neural Network. \n",
    "\n",
    "As a reminder of how an FNN functions:\n",
    "1. The network is split into a series of *layers*\n",
    "2. Each layer contains a series of nodes\n",
    "3. Each layer:\n",
    "\t1. Takes an input vector\n",
    "\t2. Performs a non-linear transformation on it (determined by the *weights* of the layer, and the activation function)\n",
    "\t3. Outputs another vector. The dimension of the output vector is equal to the number of *nodes* in the *layer*\n",
    "4. The layers take the vector given to them by the layer below, transform it, and pass it on to the next layer.\n",
    "5. The act of 'training' the network is optimising the weights of all of the nodes, in order to minimise an 'objective function'\n",
    "\n",
    "We will use the tensorflow interface, because it is almost trivially easy!\n",
    "\n",
    "<font color=blue>**What considerations should we make when deciding on the topology of our network? Is there such a thing as *too much*?**</font>\n",
    "\n",
    "<font color=red>**Choose the structure of your neural network - you can add in additional layers, change the size -- anything you want!**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9038c341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "outputDimension = 3 ## We have 3 categories and are using 1-hot encoding\n",
    "model = tf.keras.models.Sequential([\n",
    "\ttf.keras.layers.Dense(len(x_train[0]), activation='relu'),\n",
    "\ttf.keras.layers.Dense(10, activation='relu'),\n",
    "\ttf.keras.layers.Dense(outputDimension), \n",
    "\ttf.keras.layers.Softmax() #this final layer converts values -inf -> + inf into probabilities\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79529c3",
   "metadata": {},
   "source": [
    "## The Loss Function\n",
    "\n",
    "In order to decide what the `best` set of parameters is, the model needs some way of converting a *prediction* into a *score*. \n",
    "\n",
    "The standard way to do this for a classification problem is `CrossEntropy` - which is a fancy way of saying that we are comparing two probability distributions. If a value is predicted to occur with probability $p_i$ and is observed to occur with probability $q_i$, then the cross-entropy is:\n",
    "$$ H(p,q) = \\sum_i p_i \\ln(q_i)$$\n",
    "In the case of a simple classification schema, where the input is the `truth label` and a prediction vector, it amounts to:\n",
    "\n",
    "```python\n",
    "def loss_fn(truth, prediction):\n",
    "\treturn np.log(prediction[truth])\n",
    "```\n",
    "\n",
    "Tensorflow/keras nicely provide us with a highly optimised implementation of this.\n",
    "\n",
    "<font color=blue>**Why might we not want to use an out-of-the-box loss function?**</font>\n",
    "\n",
    "<font color=red>*If you have experience with Tensorflow, or have lots of time, try writing your own loss function, and see how it compares*</font>\n",
    "\n",
    "We are now ready to train our model!\n",
    "\n",
    "<font color=red>**Run the code below to train our model - it should converge quickly.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23044caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "epoch_count = 10\n",
    "M = model.fit(x_train, y_train, epochs=epoch_count, validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd2daeb",
   "metadata": {},
   "source": [
    "Now that we have trained our model, let's apply it to some data not in the training *or* the validation set -- the Tasmanian Devils (or the Human!) we were using `deforest` on.\n",
    "\n",
    "<font color=red>**Apply the trained model to your `optimalPath` output from `deforest`**</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae0dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "myEnc = optimalPath.Encode(s,N.JumpSize,'sum',100)\n",
    "\n",
    "def Prediction(model,encoding):\n",
    "\n",
    "\tpred = model.predict([encoding],verbose=0)\n",
    "\t\n",
    "\tindexSort = np.argsort(pred[0])[::-1]\n",
    "\n",
    "\ttypes = ['Normal','DFTD1','DFTD2']\n",
    "\tprint(\"Model Predictions:\")\n",
    "\tfor i in range(len(indexSort)):\n",
    "\t\tprint(f\"{types[indexSort[i]]}:\\t{pred[0][indexSort[i]]*100:.4f}%\")\n",
    "\n",
    "print(\"Input file:\",file)\n",
    "Prediction(model,myEnc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafb912d",
   "metadata": {},
   "source": [
    "<font color=blue>**Did you get the expected result?**</font> \n",
    "\n",
    "Some answers:\n",
    "\n",
    "1. Abigail and Arthur were non-cancerous samples\n",
    "2. Diane and Vern have DFTD1\n",
    "3. Cory and Elmer have DFTD2\n",
    "\n",
    "<font color=blue>**If the model predicted incorrectly, why do you think that might have been? Is it an issue?**</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440fcc7",
   "metadata": {},
   "source": [
    "# Unsupervised Methods\n",
    "\n",
    "The classifier model is great - if we set it up properly we can get upwards of 95% accuracy in our classifier, which is excellent if we were targeting a clinical application where we needed to know quickly what the cancer type was. \n",
    "\n",
    "However, it required us to possess a labelled dataset in order to train the model. Anything 'weird' inherently went poorly. \n",
    "\n",
    "What if we wanted to *discover* new types of cancer? This falls into the realm of unsupervised methods: novelty detection and the like. \n",
    "\n",
    "The method we will demonstrate here is not actually a Machine Learning model, but is close enough to fall into the same bracket: we will be using UMAP, which is a non-linear dimensionality reduction technique (similar to PCA and t-SNE, but more advanced and generally more interesting).\n",
    "\n",
    "UMAP takes high dimensional space, and projects it down into a lower dimensional space (in this, 2D), whilst attempting to keep similar vectors in the high dimensional space close to each other in the low dimensional space.\n",
    "\n",
    "In short, it attempts to group vectors by how 'similar' they are. When we start, the model has no information about any existing classifications. \n",
    "\n",
    "<font color=red>**Try running UMAP on some of the datafiles found in the `ProcessedData/`` directory.**</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap.umap_ as umap\n",
    "umapfile =  \"ProcessedData/AllData/Res_1000_diff.dat\"\n",
    "\n",
    "\n",
    "\n",
    "(data_x,data_y,names) = deforest.loadUMAPData(umapfile)\n",
    "scaled_data = StandardScaler().fit_transform(data_x) #this converts the array into standard-deviations from the mean, which means the model doesn't have to spend effort learning the mean!\n",
    "\n",
    "reducer = umap.UMAP().fit(scaled_data)\n",
    "embedding = reducer.embedding_\n",
    "plt.figure(5)\n",
    "plt.clf()\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "\ts=10,\n",
    "    c=data_y)\n",
    "\n",
    "# This function formatter will replace integers with target names\n",
    "formatter = plt.FuncFormatter(lambda val, loc: [\"Normal\",\"DFTD1\",\"DFTD2\"][int(val)])\n",
    "\n",
    "# We must be sure to specify the ticks matching our target names\n",
    "plt.colorbar(ticks=[0, 1, 2], format=formatter)\n",
    "\n",
    "\n",
    "## Enable this to add annotations to the plot\n",
    "# for i in range(len(names)):\n",
    "# \tplt.annotate(names[i],(embedding[i,0],embedding[i,1]),size=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117484f3",
   "metadata": {},
   "source": [
    "<font color=blue>**Did that work as expected? What different considerations do we have when running UMAP vs. running our classifier?**</font> \n",
    "\n",
    "# Conclusions\n",
    "\n",
    "Thank you for attending this course - I hope you found it enjoyable and engaging. \n",
    "\n",
    "To summarise what we achieved today:\n",
    "\n",
    "1. We examined a set of cancer data associated with Tasmanian Devil Facial Tumour Disease\n",
    "2. We utilised a data reduction technique - `deforest` - to encode the data into a useful feature vector\n",
    "3. We trained a simple FNN which was (I hope!) able to classify some unknown data, based on some known, pre-labelled data.\n",
    "4. We utilised a dimensionality reduction technique - `umap` - to examine the data without needing a pre-labelled training set\n",
    "\n",
    "Along the way, I hope you have found the discussions interesting and engaging, and have a good idea of the services that we at HPA provide to Sanger. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
